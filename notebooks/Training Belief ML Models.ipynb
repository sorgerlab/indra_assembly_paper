{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: [2021-05-14 09:09:59] indra_db.databases - Database postgresql://tester:***@indradb-refresh.cwcetxbvbgrf.us-east-1.rds.amazonaws.com/indradb_test is not available: (psycopg2.OperationalError) timeout expired\n",
      "\n",
      "(Background on this error at: http://sqlalche.me/e/e3q8)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import random\n",
    "from os.path import join\n",
    "from copy import deepcopy, copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from numpy import interp\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, cross_validate, StratifiedKFold\n",
    "from sklearn.metrics import roc_curve, auc, classification_report, precision_recall_curve\n",
    "\n",
    "from indra.tools import assemble_corpus as ac\n",
    "from indra.belief.skl import CountsScorer\n",
    "from bioexp.util import format_axis, fontsize\n",
    "from bioexp.curation.belief_models import OrigBeliefStmt\n",
    "from bioexp.curation.classifiers import BinaryRandomForest, LogLogisticRegression, BeliefModel\n",
    "\n",
    "from bioexp.curation.process_curations import get_full_curations, get_raw_curations, reader_input\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: [2021-05-14 09:09:59] indra.tools.assemble_corpus - Loading ../data/bioexp_asmb_preassembled.pkl...\n",
      "INFO: [2021-05-14 09:10:54] indra.tools.assemble_corpus - Loaded 895580 statements\n"
     ]
    }
   ],
   "source": [
    "# Load pickle of assembled statements.\n",
    "all_stmts = ac.load_statements('../data/bioexp_asmb_preassembled.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a dictionary so we can retrieve statements by their hash, and define some other useful variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stmts_by_hash = {s.get_hash(): s for s in all_stmts}\n",
    "\n",
    "reader_list = ['reach', 'sparser', 'medscan', 'rlimsp', 'trips']\n",
    "\n",
    "opath = '../output/'\n",
    "prefix = 'fig4_ipynb'\n",
    "\n",
    "def fig_path(name, fmt):\n",
    "    return join(opath, f'{prefix}_{name}.{fmt}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the curated data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     3
    ]
   },
   "outputs": [],
   "source": [
    "#curation_data_file = join(opath, 'curation_dataset_with_bg_psp.pkl')\n",
    "#curation_data_file = join(opath, 'curation_dataset.pkl')\n",
    "curation_data_file = join(opath, 'curation_dataset_inc.pkl')\n",
    "\n",
    "def load_curation_data(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        dataset = pickle.load(f)\n",
    "        df = pd.DataFrame.from_records(dataset)\n",
    "        df = df.fillna(0)\n",
    "    # Every column except agent names and stmt type should be int\n",
    "    dtype_dict = {col: 'int64' for col in df.columns\n",
    "                  if col not in ('agA_name', 'agA_ns', 'agA_id', 'stmt_type',\n",
    "                                 'agB_name', 'agB_ns', 'agB_id')}\n",
    "    df = df.astype(dtype_dict)\n",
    "    return df\n",
    "\n",
    "cur_df = load_curation_data(curation_data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many records in the dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1330, 21)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cur_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get a list of all the curated statements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Get dataset of curated statements along with correctness values\n",
    "def stmts_for_df(df, stmts_by_hash):\n",
    "    stmt_list = []\n",
    "    for row in df.itertuples():\n",
    "        stmt_hash = row.stmt_hash\n",
    "        stmt_list.append(stmts_by_hash[stmt_hash])\n",
    "    return stmt_list\n",
    "\n",
    "cur_stmts = stmts_for_df(cur_df, stmts_by_hash)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List all the source APIs for curated statements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['trips',\n",
       " 'hprd',\n",
       " 'rlimsp',\n",
       " 'sparser',\n",
       " 'bel',\n",
       " 'biopax',\n",
       " 'trrust',\n",
       " 'isi',\n",
       " 'medscan',\n",
       " 'signor',\n",
       " 'reach']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_sources = list(set([ev.source_api for stmt in cur_stmts for ev in stmt.evidence]))\n",
    "all_sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define classes for storing the results of training and testing models. `TrainTestResult` Records the predictions, probabilities, RO curve, PR curve, etc. for a single fold of the data. `ModelResult` aggregates the `TrainTestResult` objects for all of the folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     20
    ]
   },
   "outputs": [],
   "source": [
    "class TrainTestResult:\n",
    "    def __init__(self, y_preds, y_probs, y_test, base_fpr):\n",
    "        self.y_preds = y_preds\n",
    "        self.y_probs = y_probs      \n",
    "        self.y_test = y_test\n",
    "        # Get and store true/false positive rates and thresholds for ROC curve\n",
    "        self.fpr, self.tpr, self.thresholds = roc_curve(y_test, y_probs[:, 1])\n",
    "        # Calculate ROC AUC\n",
    "        self.roc_auc = auc(self.fpr, self.tpr)\n",
    "        # Get TPR interpolated to base_fpr\n",
    "        self.tpr_interp = interp(base_fpr, self.fpr, self.tpr)\n",
    "        self.tpr_interp[0] = 0.0\n",
    "        # Precision/recall/thresholds for each model\n",
    "        self.precision, self.recall, self.thresholds = precision_recall_curve(y_test, y_probs[:, 1])\n",
    "        self.pr_auc = auc(self.recall, self.precision)\n",
    "        #pr_auc_arr[fold_ix] = pr_auc\n",
    "        self.prec_interp = interp(base_fpr, self.thresholds, self.precision[:-1])\n",
    "        #prec_arr[fold_ix, :] = interp(base_fpr, thresholds, precision[:-1])\n",
    "        self.rec_interp = interp(base_fpr, self.thresholds, self.recall[:-1])\n",
    "\n",
    "class ModelResults:\n",
    "    def __init__(self, clf_name, feat_set_name, feat_kwargs, base_fpr):\n",
    "        # True positives\n",
    "        self.base_fpr = base_fpr\n",
    "        self.clf_name = clf_name\n",
    "        self.feat_set_name = feat_set_name\n",
    "        self.feat_kwargs = feat_kwargs\n",
    "        self.tt_results = []\n",
    "        \n",
    "    def add_result(self, tt_result):\n",
    "        self.tt_results.append(tt_result)\n",
    "\n",
    "    def get_summary(self):\n",
    "        clf_results = [{'y_preds': tt.y_preds,\n",
    "                        'y_probs': tt.y_probs,\n",
    "                        'y_test': tt.y_test} for tt in self.tt_results]\n",
    "        # Combine get matrices with all TPR, Precision and Recall results\n",
    "        dim = (len(self.tt_results), len(self.base_fpr))\n",
    "        tpr_arr = np.zeros(dim)\n",
    "        prec_arr = np.zeros(dim)\n",
    "        rec_arr = np.zeros(dim)\n",
    "        for ix, ttr in enumerate(self.tt_results):\n",
    "            tpr_arr[ix, :] = ttr.tpr_interp\n",
    "            prec_arr[ix, :] = ttr.prec_interp\n",
    "            rec_arr[ix, :] = ttr.rec_interp\n",
    "        return {\n",
    "         'clf': clf_results,\n",
    "         'roc': tpr_arr.mean(axis=0),\n",
    "         'roc_auc': np.array([ttr.roc_auc for ttr in self.tt_results]),\n",
    "         'prec': prec_arr.mean(axis=0),\n",
    "         'rec': rec_arr.mean(axis=0),\n",
    "         'pr_auc': np.array([ttr.pr_auc for ttr in self.tt_results]),\n",
    "         'x_interp': self.base_fpr}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def shuffle_train_df(df, stmts_by_hash, seed=1):\n",
    "    \"\"\"Given dataframe, return shuffled stmts and y_arr.\"\"\"\n",
    "    stmts = stmts_for_df(df, stmts_by_hash)\n",
    "    y_arr = df['correct'].values\n",
    "    return shuffle_train_stmts(stmts, y_arr, seed)\n",
    "\n",
    "def shuffle_train_stmts(stmts, y_arr, seed=1):\n",
    "    \"\"\"Return training data as a shuffled list of statements\n",
    "    and corresponding correctness values.\"\"\"\n",
    "    random.seed(seed)\n",
    "    stmt_y_pairs = list(zip(stmts, y_arr))\n",
    "    random.shuffle(stmt_y_pairs)\n",
    "    stmts, y_vals = list(zip(*stmt_y_pairs))\n",
    "    return stmts, np.array(y_vals)    \n",
    "\n",
    "# DEPRECATED. Use StratifiedKFold\n",
    "def get_split_points(size, num_folds):\n",
    "    \"\"\"Generates size/p pairs of split indices for separating an array of the given\n",
    "    size into chunks with percentage p. For example for a list of size 100 with\n",
    "    p = 0.2, returns ((0, 20), (20, 40), (40, 60), (60, 80), and (80, 100))\n",
    "    \"\"\"\n",
    "    # rsp: Raw split points\n",
    "    rsp = np.linspace(0, size, num_folds)\n",
    "    split_indices = [(int(round(rsp[i])), int(round(rsp[i+1])))\n",
    "                     for i in range(len(rsp)-1)]\n",
    "    return split_indices\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a dictionary of models and a dataset, this function:\n",
    "* Filters the dataset columns to only the given readers, and filters the rows to only those where there is evidence from those readers (i.e., so all databases are excluded)\n",
    "* Defines a set of different statement features to use\n",
    "* Splits the dataset into test and train using StratifiedKFold\n",
    "* Trains and tests all models, for all feature sets, for all folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train and evaluate a set of models\n",
    "def eval_models_relation(model_dict, df, readers, split_func, num_folds=10,\n",
    "                         cols_to_include=None, seed=1):\n",
    "    # Filter to columns where at least one of the readers has an entry\n",
    "    # Exclude\n",
    "    cols_to_drop = [col for col in df.columns if col not in readers + ['correct']]\n",
    "    df = df.drop(cols_to_drop, axis=1)\n",
    "    df = df[df[readers].any(1)]\n",
    "    print(\"Readers\", str(readers), \"Num_rows\", len(df), \"Pct corr\", df['correct'].mean())\n",
    "    # Predictors\n",
    "    predictors = {'': {'use_stmt_type': False, 'use_num_members': False}, #'All features': x,\n",
    "                  #'+ Type': {'use_stmt_type': True, 'use_num_members': False}, \n",
    "                  #'+ #Memb': {'use_stmt_type': False, 'use_num_members': True},\n",
    "                  #'+ Type/#Memb': {'use_stmt_type': True, 'use_num_members': True},\n",
    "                  #'+ #PMIDs': {'use_stmt_type': False, 'use_num_pmids': True},\n",
    "                  #'+ Type/#PMIDs': {'use_stmt_type': True, 'use_num_pmids': True},                  \n",
    "                  #'Only KGE': x[:, -1:]} # Only the score column\n",
    "                 }\n",
    "    # The baseline y-axis for ROC and PRC plots\n",
    "    base_fpr = np.linspace(0, 1, 101)    \n",
    "    # Shuffle the data\n",
    "    stmts, y_arr = shuffle_train_df(df, stmts_by_hash, seed=seed)\n",
    "    # Get split points\n",
    "    #split_indices = get_split_points(len(df), num_folds)\n",
    "    skf = StratifiedKFold(num_folds, shuffle=False)\n",
    "    skf.split(stmts, y_arr)\n",
    "    # Dict to save results\n",
    "    model_results = {}    \n",
    "    # For every fold...\n",
    "    for fold_ix, (train_ix, test_ix) in tqdm(enumerate(skf.split(stmts, y_arr))):\n",
    "        # ...try every classifier\n",
    "        for clf_name, clf in model_dict.items():\n",
    "            # ...with different sets of features (predictors):\n",
    "            for feat_set_name, feat_kwargs in predictors.items():\n",
    "                # Create a CountsModel for this type of classifier\n",
    "                model = CountsScorer(clf, readers, **feat_kwargs)\n",
    "                #model = EvidenceModel(clf) \n",
    "                # Identifier for this model type\n",
    "                model_key = '%s %s' % (clf_name, feat_set_name)\n",
    "                # If necessary, instantiate the ModelResults object to\n",
    "                # save train/test results\n",
    "                if model_key not in model_results:\n",
    "                    model_results[model_key] = ModelResults(clf_name, feat_set_name, feat_kwargs, base_fpr)\n",
    "                # Use instance of CountsModel to get feature data as a matrix\n",
    "                # with appropriate featurization\n",
    "                stmt_arr = model.to_matrix(stmts)\n",
    "                # Split the preprocessed data matrix according to the split points\n",
    "                x_test = stmt_arr[test_ix]\n",
    "                x_train = stmt_arr[train_ix]\n",
    "                y_test = y_arr[test_ix]\n",
    "                y_train = y_arr[train_ix]\n",
    "                # Train the model\n",
    "                clf.fit(x_train, y_train)\n",
    "                # Make predictions for the test set\n",
    "                y_preds = clf.predict(x_test)\n",
    "                y_probs = clf.predict_proba(x_test)\n",
    "                # Save results in TrainTestResult object\n",
    "                tt_result = TrainTestResult(y_preds, y_probs, y_test, base_fpr)\n",
    "                # Add to the model results for this model type\n",
    "                model_results[model_key].add_result(tt_result)\n",
    "    return model_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot ROC and PRC curves, averaged across folds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Function to plot mean ROC from k-fold cross-validation\n",
    "def plot_roc(results):\n",
    "    fig = plt.figure(figsize=(2.0, 2.0), dpi=150)\n",
    "    ax = plt.gca()\n",
    "    lw = 0.5\n",
    "    colors = ['r', 'g', 'b', 'orange', 'k', 'y', 'c', 'm']\n",
    "    colors = colors + colors\n",
    "    # Plot ROC curve for Belief Model (REACH)\n",
    "    #plt.plot(bel_fpr, bel_tpr, color=colors[0],\n",
    "    #         lw=lw, label='Orig. Belief (area = %0.2f)' % bel_roc_auc)\n",
    "    #roc_results = results['roc']\n",
    "    #roc_auc = results['roc_auc']\n",
    "    for i, (clf_name, mr_obj) in enumerate(results.items()):\n",
    "        mr_dict = mr_obj.get_summary()\n",
    "        roc_auc_arr = mr_dict['roc_auc']\n",
    "        plt.plot(mr_dict['x_interp'], mr_dict['roc'], color=colors[i],\n",
    "             lw=lw, label='%s (area = %0.3f +/- %0.3f)' %\n",
    "                          (clf_name, roc_auc_arr.mean(), roc_auc_arr.std() / np.sqrt(len(roc_auc_arr))))\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "    plt.xlim([0.0, 1.01])\n",
    "    plt.ylim([0.0, 1.01])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic', fontsize=6)\n",
    "    plt.legend(loc=\"lower right\", fontsize=4, frameon=False)\n",
    "    format_axis(ax)\n",
    "    \n",
    "    plt.subplots_adjust(left=0.15)\n",
    "    return fig\n",
    "\n",
    "# Function to plot mean precision-recall curve from k-fold cross validation\n",
    "def plot_prc(results):\n",
    "    fig = plt.figure(figsize=(2.0, 2.0), dpi=150)\n",
    "    ax = plt.gca()\n",
    "    lw = 0.5\n",
    "    colors = ['r', 'g', 'b', 'orange', 'k', 'y', 'c', 'm']\n",
    "    colors = colors + colors\n",
    "    for i, (clf_name, mr_obj) in enumerate(results.items()):\n",
    "        mr_dict = mr_obj.get_summary()\n",
    "        prec_arr = mr_dict['prec']\n",
    "        rec_arr = mr_dict['rec']\n",
    "        pr_auc = mr_dict['pr_auc']                                   \n",
    "        plt.plot(rec_arr, prec_arr, color=colors[i],\n",
    "             lw=lw, label='%s (area = %0.3f +/- %0.3f)' %\n",
    "                          (clf_name, pr_auc.mean(), pr_auc.std() / np.sqrt(len(pr_auc))))\n",
    "    plt.xlim([0.0, 1.01])\n",
    "    plt.ylim([0.0, 1.01])\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    fontsize=6\n",
    "    plt.title('Precision-Recall Curve', fontsize=6)\n",
    "    plt.legend(loc=\"lower left\", fontsize=4, frameon=False)\n",
    "    format_axis(ax)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = kge_df\n",
    "readers = ['sparser']\n",
    "cols_to_drop = [col for col in df.columns if col not in readers + ['correct']]\n",
    "df = df.drop(cols_to_drop, axis=1)\n",
    "df = df[df[readers].any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1078 - (124 + 383)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df['reach'] > 0) | (df['sparser'] > 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = kge_df\n",
    "df[(df['reach'] > 0) | (df['sparser'] > 0)].correct.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(518 + 82) / (571 + 124)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ROC and PRC comparison for different models, using all data\n",
    "#train_df = kge_df[reader_list + ['correct']]\n",
    "models_ev = {\n",
    "    #'LR': LogisticRegression(),\n",
    "    'Log LR': LogLogisticRegression(),    \n",
    "    #'RF d5': RandomForestClassifier(n_estimators=1000, max_depth=6),\n",
    "    #'RF d6': RandomForestClassifier(n_estimators=1000, max_depth=7),\n",
    "    #'Random Forest': RandomForestClassifier(),\n",
    "    #'Binary RF n1000d4': BinaryRandomForest(n_estimators=1000, max_depth=4),\n",
    "    #'Binary RF': BinaryRandomForest(),\n",
    "    #'Binary RF n1000': BinaryRandomForest(n_estimators=1000)\n",
    "    #'Belief Orig': BeliefModel(reader_list),\n",
    "    #'Belief Binomial': BeliefModel(reader_subset, model_class=BinomialStmt),\n",
    "    #'BeliefBayes': BeliefBayesModel(train_df, 0, reader_subset, model_class=None,\n",
    "    #                           nwalkers=100, burn_steps=100, sample_steps=100)\n",
    "    #'GaussianNB': GaussianNB(),\n",
    "}\n",
    "\n",
    "reader_list = ['rlimsp', 'trips', 'sparser']\n",
    "res_ev = eval_models_relation(models_ev, kge_df, reader_list, None,\n",
    "                              num_folds=10, cols_to_include=None, seed=1)\n",
    "#res_ev = eval_models_relation(models_ev, ev_stmts, reader_list, train_test_split,\n",
    "#                              num_folds=10, cols_to_include=None, seed=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plot_roc(res_ev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plot_prc(res_ev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We note that the performance of all classifiers increases with the number of readers used. We measured AUROC and AUPRC of each classifier starting with data from 1 reader and increasing to 5 and not surprisingly found that both increased with additional readers. It is important to note though that this analysis was performed using the unweighted curation dataset which is biased towards high-mention statements with a higher likelihood of reader overlap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_aurocs(clf_aurocs, xticks, xticklabels, title, xlabel):\n",
    "    fig = plt.figure(figsize=(2.5, 2), dpi=150)\n",
    "    for clf_name, aurocs in clf_aurocs.items():\n",
    "        plt.errorbar(xticks, aurocs['mean'], yerr=aurocs['se'], label=clf_name, marker='.',\n",
    "                     lw=0.5)\n",
    "    plt.legend(loc=\"lower right\", frameon=False, fontsize=6)\n",
    "    plt.ylabel('AUROC')\n",
    "    #plt.title(title)\n",
    "    plt.xticks(xticks)\n",
    "    plt.xlabel(xlabel)\n",
    "    ax = plt.gca()\n",
    "    ax.set_xticklabels(xticklabels)\n",
    "    plt.subplots_adjust(left=0.18, right=0.95)\n",
    "    format_axis(ax)\n",
    "    return fig\n",
    "\n",
    "def plot_auprcs(clf_auprcs, xticks, xticklabels, title, xlabel):\n",
    "    fig = plt.figure(figsize=(2.5, 2), dpi=150)\n",
    "    for clf_name, auprcs in clf_auprcs.items():\n",
    "        plt.errorbar(xticks, auprcs['mean'], yerr=auprcs['se'], label=clf_name, marker='.',\n",
    "                     lw=0.5)\n",
    "    plt.legend(loc=\"lower right\", frameon=False, fontsize=6)\n",
    "    plt.ylabel('AUPRC')\n",
    "    #plt.title(title)\n",
    "    plt.xticks(xticks)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylim([0.75, 1.0])\n",
    "    ax = plt.gca()\n",
    "    ax.set_xticklabels(xticklabels)\n",
    "    plt.subplots_adjust(left=0.18, right=0.95)\n",
    "    format_axis(ax)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def adding_readers(reader_list, df, num_folds=10):\n",
    "    clf_aurocs = {}\n",
    "    clf_auprcs = {}\n",
    "    for i in range(1, len(reader_list) + 1):\n",
    "        import ipdb; ipdb.set_trace()\n",
    "        reader_subset = reader_list[0:i]\n",
    "        cols_to_drop = [col for col in df.columns if col not in reader_subset + ['correct']]\n",
    "        #readers_to_drop = reader_list[i:]\n",
    "        train_df = df.drop(cols_to_drop, axis=1)\n",
    "        train_df = train_df[train_df[reader_subset].any(1)]\n",
    "        print(f\"Readers: {reader_subset}; num_rows: {len(train_df)}; correct: {train_df.correct.mean()}\")\n",
    "        models_ev = {\n",
    "            #'Logistic Regression': LogisticRegression(),\n",
    "            'Log-Logistic Regression': LogLogisticRegression(),    \n",
    "            #'Random Forest': RandomForestClassifier(n_estimators=1000, max_depth=6),\n",
    "            #'Random Forest': RandomForestClassifier(),\n",
    "            #'Binary Random Forest': BinaryRandomForest(n_estimators=1000, max_depth=4),\n",
    "            #'Binary Random Forest': BinaryRandomForest(),\n",
    "            #'Belief': BeliefModel(reader_subset),\n",
    "            #'BeliefBayes': BeliefBayesModel(train_df, 0, reader_subset, model_class=None,\n",
    "            #                           nwalkers=100, burn_steps=100, sample_steps=100)\n",
    "            #'GaussianNB': GaussianNB(),\n",
    "        }\n",
    "                \n",
    "        res_ev = eval_models_relation(models_ev, train_df, reader_subset, None,\n",
    "                                      num_folds=num_folds)\n",
    "        for i, (clf_name, clf_res) in enumerate(res_ev.items()):\n",
    "            clf_summary = clf_res.get_summary()\n",
    "            roc_auc_arr = clf_summary['roc_auc']\n",
    "            pr_auc_arr = clf_summary['pr_auc']\n",
    "            if clf_name not in clf_aurocs:\n",
    "                clf_aurocs[clf_name] = {'mean': [], 'se': []}                \n",
    "            clf_aurocs[clf_name]['mean'].append(roc_auc_arr.mean())\n",
    "            clf_aurocs[clf_name]['se'].append(roc_auc_arr.std() / np.sqrt(len(roc_auc_arr)))\n",
    "            if clf_name not in clf_auprcs:\n",
    "                clf_auprcs[clf_name] = {'mean': [], 'se': []}\n",
    "            clf_auprcs[clf_name]['mean'].append(pr_auc_arr.mean())\n",
    "            clf_auprcs[clf_name]['se'].append(pr_auc_arr.std())\n",
    "    return (clf_aurocs, clf_auprcs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding readers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader_list = ['reach', 'medscan', 'sparser', 'trips',] #'rlimsp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#ar_aurocs, ar_auprcs = adding_readers(reader_list, kge_df, num_folds=10)\n",
    "ar_aurocs, ar_auprcs = adding_readers(reader_list, kge_df, num_folds=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Readers: ['reach']; num_rows: 1007; correct: 0.8083416087388282\n",
    "* Readers: ['reach', 'medscan']; num_rows: 1176; correct: 0.7882653061224489\n",
    "* Readers: ['reach', 'medscan', 'sparser']; num_rows: 1308; correct: 0.7408256880733946\n",
    "* Readers: ['reach', 'medscan', 'sparser', 'trips']; num_rows: 1370; correct: 0.7372262773722628\n",
    "* Readers: ['reach', 'medscan', 'sparser', 'trips', 'rlimsp']; num_rows: 1435; correct: 0.7344947735191638"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_aurocs(ar_aurocs, list(range(len(reader_list))), reader_list,\n",
    "            'Classifier AUROC with increasing readers', 'Reader added')\n",
    "fig.savefig(fig_path('auroc_more_readers', 'pdf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_auprcs(ar_auprcs, list(range(len(reader_list))), reader_list,\n",
    "           'Classifier AUPRC with increasing readers', 'Reader added')\n",
    "fig.savefig(fig_path('auprc_more_readers', 'pdf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def adding_data(num_low_high, reader_list, df, num_folds=10):\n",
    "    clf_aurocs = {}\n",
    "    clf_auprcs = {}\n",
    "    \n",
    "    for i, (num_low, num_high) in enumerate(num_low_high):\n",
    "        reader_subset = reader_list[0:i]\n",
    "        readers_to_drop = reader_list[i:]\n",
    "        train_df = df.drop(readers_to_drop, axis=1)\n",
    "        \n",
    "        def split_closure(x_data, y_data, test_size):\n",
    "            return split_by_ev_ct(x_data, y_data, reader_list, low_ub=1, num_low=num_low,\n",
    "                                  high_lb=8, num_high=num_high, test_size=test_size)\n",
    "        \n",
    "        models_ev = {\n",
    "            'Logistic Regression': LogisticRegression(),\n",
    "            'Log-Logistic Regression': LogLogisticRegression(),    \n",
    "            #'Random Forest': RandomForestClassifier(n_estimators=1000, max_depth=4),\n",
    "            #'Random Forest': RandomForestClassifier(),\n",
    "            #'Binary Random Forest': BinaryRandomForest(n_estimators=1000, max_depth=4),\n",
    "            #'Belief': BeliefModel(reader_subset),\n",
    "            #'BeliefBayes': BeliefBayesModel(train_df, 0, reader_subset, model_class=None,\n",
    "            #                           nwalkers=100, burn_steps=100, sample_steps=100)\n",
    "            #'GaussianNB': GaussianNB(),\n",
    "        }\n",
    "        res_ev = eval_models_relation(models_ev, train_df, reader_list, split_closure,\n",
    "                                      test_split=0.2, num_folds=num_folds)\n",
    "        roc_auc = res_ev['roc_auc']\n",
    "        pr_auc = res_ev['pr_auc']\n",
    "        for i, (clf_name, roc_auc_arr) in enumerate(roc_auc.items()):\n",
    "            pr_auc_arr = pr_auc[clf_name]\n",
    "            if clf_name not in clf_aurocs:\n",
    "                clf_aurocs[clf_name] = {'mean': [], 'sd': []}\n",
    "            clf_aurocs[clf_name]['mean'].append(roc_auc_arr.mean())\n",
    "            clf_aurocs[clf_name]['sd'].append(roc_auc_arr.std())\n",
    "            if clf_name not in clf_auprcs:\n",
    "                clf_auprcs[clf_name] = {'mean': [], 'sd': []}\n",
    "            clf_auprcs[clf_name]['mean'].append(pr_auc_arr.mean())\n",
    "            clf_auprcs[clf_name]['sd'].append(pr_auc_arr.std())\n",
    "    return (clf_aurocs, clf_auprcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_low_high = [(5, 5), (10, 10), (20, 20), (30, 30)]\n",
    "ad_aurocs, ad_auprcs = adding_data(num_low_high, reader_list, kge_df, num_folds=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stmts_for_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Include RGCN Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rgcn_df = pd.read_csv('curated_with_scores.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgcn_df.score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
